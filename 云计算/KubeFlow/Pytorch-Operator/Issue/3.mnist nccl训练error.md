<!-- TOC -->

- [1. 问题描述](#1-问题描述)
    - [1.1 集群](#11-集群)
- [2. 问题分析](#2-问题分析)
    - [2.1 执行过程](#21-执行过程)
    - [2.2 对比gloo和nccl](#22-对比gloo和nccl)
    - [2.3 验证GPU问题](#23-验证gpu问题)

<!-- /TOC -->
# 1. 问题描述
* mnist example 在就集群是可以通过的，但是重新构建集群后发现训练失败
![](./images/2020-04-07-13-47-24.png)
![](./images/2020-04-07-13-48-17.png)

## 1.1 集群
* 分别操作过sense-rubber和pytorch-operator集群

# 2. 问题分析
## 2.1 执行过程
* sense-rubber集群在容器中执行import Torch失败（但是pytorch集群没出现问题）
![](./images/2020-04-07-13-50-23.png)

* issue描述是mkl-dnn bug 导致pytorch 异常。解决方式是升级mkl-dnn或者pytorch
https://github.com/pytorch/pytorch/issues/16183

* 在pytorch集群执行同样的nccl job也异常
![](./images/2020-04-07-13-59-25.png)

* 手动执行nccl，返现cuda device异常
![](./images/2020-04-07-14-00-17.png)

* 查看cuda device，发现无可用的cuda device
![](./images/2020-04-07-14-04-17.png)

## 2.2 对比gloo和nccl
gloo通信成功，但是nccl在执行nn.parallel.DistributedDataParallel同步时失败。
* gloo 主要支持cpu，nccl只支持gpu，因此我们认为GPU通信出现问题
![](./images/2020-04-07-14-25-53.png)

## 2.3 验证GPU问题
* 显然在容器中执行nvml失败，不符合要求
![](./images/2020-04-07-14-27-39.png)
* 同时在集群中没发现nvidia-docker
![](./images/2020-04-07-14-29-26.png)

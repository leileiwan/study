<!-- TOC -->

- [1. 背景](#1-背景)
- [2. cuda编程模型基础](#2-cuda编程模型基础)
    - [2.1 cuda程序执行流程](#21-cuda程序执行流程)
    - [2.2 cuda三个函数类型](#22-cuda三个函数类型)
    - [2.3 kernel线程层次结构](#23-kernel线程层次结构)
        - [2.3.1 流式处理器（SM）](#231-流式处理器sm)
    - [2.4 cuda 内存模型](#24-cuda-内存模型)
    - [2.5 GPU 硬件基本认识](#25-gpu-硬件基本认识)
        - [2.5.1 硬件配置检查](#251-硬件配置检查)
- [3. 简单cuda例子](#3-简单cuda例子)
    - [3.1 cuda 编程重要API](#31-cuda-编程重要api)
        - [3.1.1 cudaMalloc](#311-cudamalloc)
        - [3.1.2 cudaFree](#312-cudafree)
        - [3.1.3 cudaMemcpy](#313-cudamemcpy)
    - [3.2 向量相加](#32-向量相加)

<!-- /TOC -->
* reference
    * https://zhuanlan.zhihu.com/p/34587739
# 1. 背景
* 2006年，NVIDIA公司发布了CUDA，CUDA是建立在NVIDIA的GPUs上的一个通用并行计算平台和编程模型，基于CUDA编程可以利用GPUs的并行计算引擎来更加高效地解决比较复杂的计算难题。

* GPU并不是一个独立运行的计算平台，而需要与CPU协同工作，可以看成是CPU的协处理器，因此当我们在说GPU并行计算时，其实是指的基于CPU+GPU的异构计算架构

* 在异构计算架构中，GPU与CPU通过PCIe总线连接在一起来协同工作，CPU所在位置称为为主机端（host），而GPU所在位置称为设备端（device），如下图所示。
![](./images/2019-12-09-14-40-23.png)

* 可以看到GPU包括更多的运算核心，其特别适合数据并行的计算密集型任务，如大型矩阵运算，而CPU的运算核心较少，但是其可以实现复杂的逻辑运算，因此其适合控制密集型任务。另外，CPU上的线程是重量级的，上下文切换开销大，但是GPU由于存在很多核心，其线程是轻量级的。因此，基于CPU+GPU的异构计算平台可以优势互补，CPU负责处理逻辑复杂的串行程序，而GPU重点处理数据密集型的并行计算程序，从而发挥最大功效。
![](./images/2019-12-09-14-42-16.png)


# 2. cuda编程模型基础
* 在CUDA中，host和device是两个重要的概念，我们用host指代CPU及其内存，而用device指代GPU及其内存。
* CUDA程序中既包含host程序，又包含device程序，它们分别在CPU和GPU上运行。
* host与device之间可以进行通信，这样它们之间可以进行数据拷贝。
## 2.1 cuda程序执行流程
* 典型的CUDA程序的执行流程如下：
    * 分配host内存，并进行数据初始化；
    * 分配device内存，并从host将数据拷贝到device上；
    * 调用CUDA的核函数在device上完成指定的运算；
    * 将device上的运算结果拷贝到host上；
    * 释放device和host上分配的内存。
* 上面流程中最重要的一个过程是调用CUDA的核函数来执行并行计算，kernel是CUDA中一个重要的概念，kernel是在device上线程中并行执行的函数，核函数用__global__符号声明，在调用时需要用<<<grid, block>>>来指定kernel要执行的线程数量，在CUDA中，每一个线程都要执行核函数，并且每个线程会分配一个唯一的线程号thread ID，这个ID值可以通过核函数的内置变量threadIdx来获得。

## 2.2 cuda三个函数类型
由于GPU实际上是异构模型，所以需要区分host和device上的代码，在CUDA中是通过函数类型限定词开区别host和device上的函数，主要的三个函数类型限定词如下：
* \_\_global\_\_：在device上执行，从host中调用（一些特定的GPU也可以从device上调用），返回类型必须是void，不支持可变参数参数，不能成为类成员函数。注意用__global__定义的kernel是异步的，这意味着host不会等待kernel执行完就执行下一步。
* \_\_device\_\_：在device上执行，单仅可以从device中调用，不可以和__global__同时用。
* \_\_host\_\_：在host上执行，仅可以从host上调用，一般省略不写，不可以和__global__同时用，但可和__device__，此时函数会在device和host都编译。


## 2.3 kernel线程层次结构
* kernel在device上执行其实是启动很多线程，一个kernel启动的所有线程称为一个网格grid，同一个网格上的线程共享全局内存空间。

* grid是线程结构的第一层次，网格可以分成很多线程块，一个线程块中包含很多线程，这是第二层次
    * 如下图所示是一个线程的两层组织结构
    * 这是一个grid和block均是2-dim的线程组织。
    ![](./images/2019-12-09-15-21-52.png)

* grid和block都是定义为dim3类型变量，dim3可以看成包含三个无符号整数（x，y，z）  成员结构体，在定义时缺省是1

* kernel在调用时也必须通过执行配置<<<grid, block>>>来指定kernel所使用的线程数及结构。
```
dim3 grid(3, 2);
dim3 block(5, 3);
kernel_fun<<< grid, block >>>(prams...);
```

### 2.3.1 流式处理器（SM）
* 一个线程块上的线程是放在同一个流式多处理器（SM)上的，但是单个SM的资源有限，这导致线程块中的线程数是有限制的，现代GPUs的线程块可支持的线程数可达1024个。

* 有时候，我们要知道一个线程在blcok中的全局ID，此时就必须还要知道block的组织结构，这是通过线程的内置变量blockDim来获得。它获取线程块各个维度的大小。
    * 对于一个2-dim的block （Dx，Dy），线程 （x，y）的ID值为 （x+y*Dx）
* 另外线程还有内置变量gridDim，用于获得网格块各个维度的大小。

kernel的这种线程组织结构天然适合vector,matrix等运算，如我们将利用上图2-dim结构实现两个矩阵的加法，每个线程负责处理每个位置的两个元素相加，代码如下所示。线程块大小为(16, 16)，然后将N*N大小的矩阵均分为不同的线程块来执行加法运算。
```
// Kernel定义
__global__ void MatAdd(float A[N][N], float B[N][N], float C[N][N]) 
{ 
    int i = blockIdx.x * blockDim.x + threadIdx.x; 
    int j = blockIdx.y * blockDim.y + threadIdx.y; 
    if (i < N && j < N) 
        C[i][j] = A[i][j] + B[i][j]; 
}
int main() 
{ 
    ...
    // Kernel 线程配置
    dim3 threadsPerBlock(16, 16); 
    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
    // kernel调用
    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C); 
    ...
}
```

## 2.4 cuda 内存模型
* 每个线程都有自己私有本地内存，每个线程块有包含共享内存，可以被线程块中所有内存共享，其生命周期与线程块一致
* 此外所有线程都可以访问全局内存（Global Memory）。还可以访问一些只读内存块：常量内存（Constant Memory）和纹理内存（Texture Memory）
![](./images/2019-12-09-15-58-52.png)


## 2.5 GPU 硬件基本认识
* 一个kernel 会起很多线程，理论上这些线程是并行运行的的，但是物理层并不一定。
* 多线程如果没有多核支持，在物理层也是无法实现并行的
* GPU硬件一个核心硬件组件是SM，Streaming MultiProcessor
    * SM核心组件包括CUDA核心，共享内存，寄存器等，SM可以并发执行数百个线程，并发能力取决于SM所拥有的资源数
* 但给一个kernel执行时，它的grid中的线程块被分配到线程块被分配到SM上，一个线程块只能在一个SM上被调度。SM一般可以调度多个线程块，这要看SM本身的能力。
* 那么有可能一个kernel的各个线程块被分配多个SM，所以grid只是逻辑层，而SM才是执行的物理层。
* SM采用的是SIMT (Single-Instruction, Multiple-Thread，单指令多线程)架构，基本的执行单元是线程束（wraps)，线程束包含32个线程，这些线程同时执行相同的指令，但是每个线程都包含自己的指令地址计数器和寄存器状态，也有自己独立的执行路径。
    * 所以尽管线程束中的线程同时从同一程序地址执行，但是可能具有不同的行为，比如遇到了分支结构，一些线程可能进入这个分支，但是另外一些有可能不执行，它们只能死等，因为GPU规定线程束中所有线程在同一周期执行相同的指令，线程束分化会导致性能下降。
    * 当线程块被划分到某个SM上时，它将进一步划分为多个线程束，因为这才是SM的基本执行单元，但是一个SM同时并发的线程束数是有限的。这是因为资源限制，SM要为每个线程块分配共享内存，而也要为每个线程束中的线程分配独立的寄存器。
    * 所以SM的配置会影响其所支持的线程块和线程束并发数量
* 总之，就是网格和线程块只是逻辑划分，一个kernel的所有线程其实在物理层是不一定同时并发的。所以kernel的grid和block的配置不同，性能会出现差异，这点是要特别注意的。
* 由于SM的基本执行单元是包含32个线程的线程束，所以block大小一般要设置为32的倍数。
![](./images/2019-12-09-16-17-19.png)


### 2.5.1 硬件配置检查
在进行CUDA编程前，可以先检查一下自己的GPU的硬件配置，这样才可以有的放矢，可以通过下面的程序获得GPU的配置属性：
```
  int dev = 0;
    cudaDeviceProp devProp;
    CHECK(cudaGetDeviceProperties(&devProp, dev));
    std::cout << "使用GPU device " << dev << ": " << devProp.name << std::endl;
    std::cout << "SM的数量：" << devProp.multiProcessorCount << std::endl;
    std::cout << "每个线程块的共享内存大小：" << devProp.sharedMemPerBlock / 1024.0 << " KB" << std::endl;
    std::cout << "每个线程块的最大线程数：" << devProp.maxThreadsPerBlock << std::endl;
    std::cout << "每个EM的最大线程数：" << devProp.maxThreadsPerMultiProcessor << std::endl;
    std::cout << "每个EM的最大线程束数：" << devProp.maxThreadsPerMultiProcessor / 32 << std::endl;

    // 输出如下
    使用GPU device 0: GeForce GT 730
    SM的数量：2
    每个线程块的共享内存大小：48 KB
    每个线程块的最大线程数：1024
    每个EM的最大线程数：2048
    每个EM的最大线程束数：64
```


# 3. 简单cuda例子
## 3.1 cuda 编程重要API
### 3.1.1 cudaMalloc
* 在device上分配内存的cudaMalloc函数
    * 这个函数和C语言中的malloc类似，但是在device上申请一定字节大小的显存，其中devPtr是指向所分配内存的指针。
```
cudaError_t cudaMalloc(void** devPtr, size_t size);
```

### 3.1.2 cudaFree
* 释放分配的内存使用cudaFree函数

### 3.1.3 cudaMemcpy
* 负责host和device之间数据通信
```
cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind)
```
* src指向数据源，而dst是目标区域，count是复制的字节数，其中kind控制复制的方向(cudaMemcpyHostToHost, cudaMemcpyHostToDevice, cudaMemcpyDeviceToHost及cudaMemcpyDeviceToDevice)


## 3.2 向量相加
* 这里grid和block都设计为1-dim，首先定义kernel如下
```
// 两个向量加法kernel，grid和block均为一维
__global__ void add(float* x, float * y, float* z, int n)
{
    // 获取全局索引
    int index = threadIdx.x + blockIdx.x * blockDim.x;
    // 步长
    int stride = blockDim.x * gridDim.x;
    for (int i = index; i < n; i += stride)
    {
        z[i] = x[i] + y[i];
    }
}
```
* 其中stride是整个grid的线程数，有时候向量的元素数很多，这时候可以将在每个线程实现多个元素（元素总数/线程总数）的加法
* 不过下面的例子一个线程只处理一个元素，所以kernel里面的循环是不执行的。下面我们具体实现向量加法：
```
int main()
{
    int N = 1 << 20;
    int nBytes = N * sizeof(float);
    // 申请host内存
    float *x, *y, *z;
    x = (float*)malloc(nBytes);
    y = (float*)malloc(nBytes);
    z = (float*)malloc(nBytes);

    // 初始化数据
    for (int i = 0; i < N; ++i)
    {
        x[i] = 10.0;
        y[i] = 20.0;
    }

    // 申请device内存
    float *d_x, *d_y, *d_z;
    cudaMalloc((void**)&d_x, nBytes);
    cudaMalloc((void**)&d_y, nBytes);
    cudaMalloc((void**)&d_z, nBytes);

    // 将host数据拷贝到device
    cudaMemcpy((void*)d_x, (void*)x, nBytes, cudaMemcpyHostToDevice);
    cudaMemcpy((void*)d_y, (void*)y, nBytes, cudaMemcpyHostToDevice);
    // 定义kernel的执行配置
    dim3 blockSize(256);
    dim3 gridSize((N + blockSize.x - 1) / blockSize.x);
    // 执行kernel
    add << < gridSize, blockSize >> >(d_x, d_y, d_z, N);

    // 将device得到的结果拷贝到host
    cudaMemcpy((void*)z, (void*)d_z, nBytes, cudaMemcpyHostToDevice);

    // 检查执行结果
    float maxError = 0.0;
    for (int i = 0; i < N; i++)
        maxError = fmax(maxError, fabs(z[i] - 30.0));
    std::cout << "最大误差: " << maxError << std::endl;

    // 释放device内存
    cudaFree(d_x);
    cudaFree(d_y);
    cudaFree(d_z);
    // 释放host内存
    free(x);
    free(y);
    free(z);

    return 0;
}
```
* 这里我们的向量大小为1<<20，而block大小为256，那么grid大小是4096，kernel的线程层级结构如下图所示：
![](./images/2019-12-09-16-44-01.png)

* 使用nvprof工具可以分析kernel运行情况，结果如下所示，可以看到kernel函数费时约1.5ms。
```
nvprof cuda9.exe
==7244== NVPROF is profiling process 7244, command: cuda9.exe
最大误差: 4.31602e+008
==7244== Profiling application: cuda9.exe
==7244== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   67.57%  3.2256ms         2  1.6128ms  1.6017ms  1.6239ms  [CUDA memcpy HtoD]
                   32.43%  1.5478ms         1  1.5478ms  1.5478ms  1.5478ms  add(float*, float*, float*, int)
```

* 你调整block的大小，对比不同配置下的kernel运行情况，我这里测试的是当block为128时，kernel费时约1.6ms，而block为512时kernel费时约1.7ms，当block为64时，kernel费时约2.3ms。看来不是block越大越好，而要适当选择。

* 在上面的实现中，我们需要单独在host和device上进行内存分配，并且要进行数据拷贝，这是很容易出错的。好在CUDA 6.0引入统一内存（Unified Memory）来避免这种麻烦，简单来说就是统一内存使用一个托管内存来共同管理host和device中的内存，并且自动在host和device中进行数据传输。
    ```
    cudaError_t cudaMallocManaged(void **devPtr, size_t size, unsigned int flag=0);
    ```
    * 利用统一内存，可以将上面的程序简化如下：
    ```
        int main()
        {
            int N = 1 << 20;
            int nBytes = N * sizeof(float);

            // 申请托管内存
            float *x, *y, *z;
            cudaMallocManaged((void**)&x, nBytes);
            cudaMallocManaged((void**)&y, nBytes);
            cudaMallocManaged((void**)&z, nBytes);

            // 初始化数据
            for (int i = 0; i < N; ++i)
            {
                x[i] = 10.0;
                y[i] = 20.0;
            }

            // 定义kernel的执行配置
            dim3 blockSize(256);
            dim3 gridSize((N + blockSize.x - 1) / blockSize.x);
            // 执行kernel
            add << < gridSize, blockSize >> >(x, y, z, N);

            // 同步device 保证结果能正确访问
            cudaDeviceSynchronize();
            // 检查执行结果
            float maxError = 0.0;
            for (int i = 0; i < N; i++)
                maxError = fmax(maxError, fabs(z[i] - 30.0));
            std::cout << "最大误差: " << maxError << std::endl;

            // 释放内存
            cudaFree(x);
            cudaFree(y);
            cudaFree(z);

            return 0;
        }
    ```